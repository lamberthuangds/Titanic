{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.client import device_lib\n",
    "model_path = '/Users/lambert/Documents/Python_code/tensorflow_code/Titanic/Titanic_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data clearning\n",
    "# Age - find the median and fill to \n",
    "# may be try df_test_Age with its own median\n",
    "# fill_df_test_Age = df_test.Age.median()\n",
    "fill_df_Age = df_train.Age.median()\n",
    "df_train.Age.fillna(fill_df_Age, inplace=True)\n",
    "df_test.Age.fillna(fill_df_Age, inplace=True)\n",
    "\n",
    "# Cabin - fill nan with 'fill' and select first cabin\n",
    "df_train.Cabin.fillna('fill',inplace=True)\n",
    "df_train.Cabin = df_train.Cabin.map(lambda x:x.split(' ')[0])\n",
    "df_test.Cabin.fillna('fill', inplace=True)\n",
    "df_test.Cabin = df_test.Cabin.map(lambda x:x.split(' ')[0])\n",
    "\n",
    "# Sex - male: 1, female: 1\n",
    "df_train.Sex.replace({'male':1, 'female':0}, inplace=True)\n",
    "df_test.Sex.replace({'male':1, 'female':0}, inplace=True)\n",
    "\n",
    "df_train = df_train.dropna(axis=0, how='any')\n",
    "df_test.Fare.fillna(df_test.Fare.mean(), inplace=True)\n",
    "\n",
    "# Create a 'Deceased' column for sconed class\n",
    "df_train['Deceased'] = df_train.Survived.apply(lambda s: int(not s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId                            608\n",
       "Survived                                 1\n",
       "Pclass                                   1\n",
       "Name           Daniel, Mr. Robert Williams\n",
       "Sex                                      1\n",
       "Age                                     27\n",
       "SibSp                                    0\n",
       "Parch                                    0\n",
       "Ticket                              113804\n",
       "Fare                                  30.5\n",
       "Cabin                                 fill\n",
       "Embarked                                 S\n",
       "Deceased                                 0\n",
       "Name: 607, dtype: object"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Select the features\n",
    "sFeatures = ['Pclass',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',             \n",
    " 'Cabin',             \n",
    " 'Embarked']\n",
    "\n",
    "X = df_train[sFeatures]\n",
    "y = df_train[['Survived','Deceased']]\n",
    "y = y.values.reshape(len(y),2)\n",
    "X_test = df_test[sFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambert/virtualenvs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/lambert/virtualenvs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/lambert/virtualenvs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/lambert/virtualenvs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# 3. Label Encoder\n",
    "# X_test has elements that X doesn't have\n",
    "# create a Cabin_labels cover X and X_test\n",
    "X_Cabin_unique = X['Cabin'].unique()\n",
    "X_lack = X_test.Cabin[X_test['Cabin'].isin(X_Cabin_unique)==0].values\n",
    "Cabin_labels = np.append(X_Cabin_unique, X_lack)\n",
    "# print(X_Cabin_unique.shape, X_Cabin_unique)\n",
    "# print(Cabin_labels.shape, Cabin_labels)\n",
    "\n",
    "# LabelEncoder categorical Cabin and Embarked\n",
    "# le_Cabin = LabelEncoder().fit(np.array(X['Cabin'].tolist()))\n",
    "le_Cabin = LabelEncoder().fit(Cabin_labels)\n",
    "le_Embarked = LabelEncoder().fit(np.array(X['Embarked'].tolist()))\n",
    "\n",
    "X['Cabin'] = le_Cabin.transform(X['Cabin'])\n",
    "X['Embarked'] = le_Embarked.transform(X['Embarked'])\n",
    "X_test['Cabin'] = le_Cabin.transform(X_test['Cabin'])\n",
    "X_test['Embarked'] = le_Embarked.transform(X_test['Embarked'])\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.587500</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.850000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.554199</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  SibSp  Parch       Fare  Cabin  Embarked\n",
       "417     2.0  0.0  18.0    0.0    2.0  13.000000  180.0       2.0\n",
       "63      3.0  1.0   4.0    3.0    2.0  27.900000  180.0       2.0\n",
       "589     3.0  1.0  28.0    0.0    0.0   8.050000  180.0       2.0\n",
       "483     3.0  0.0  63.0    0.0    0.0   9.587500  180.0       2.0\n",
       "206     3.0  1.0  32.0    1.0    0.0  15.850000  180.0       2.0\n",
       "228     2.0  1.0  18.0    0.0    0.0  13.000000  180.0       2.0\n",
       "871     1.0  0.0  47.0    1.0    1.0  52.554199  130.0       2.0\n",
       "510     3.0  1.0  29.0    0.0    0.0   7.750000  180.0       1.0\n",
       "241     3.0  0.0  28.0    1.0    0.0  15.500000  180.0       1.0\n",
       "865     2.0  0.0  42.0    0.0    0.0  13.000000  180.0       2.0"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch, X, y):\n",
    "    num = np.arange(len(X))\n",
    "    np.random.shuffle(num)\n",
    "    select = num[0:batch]\n",
    "    x_batch = X.iloc[select]\n",
    "    y_batch = y[select]\n",
    "    \n",
    "    return(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    total_loss = 0\n",
    "    for i in range(num_iterations):\n",
    "        x_batch, y_batch = next_batch(batch_size, X_train_data, y_train_data)\n",
    "        feed_dict_train = {X_place: x_batch, y_place: y_batch}\n",
    "        _, loss = sess.run([train_op, cost], feed_dict=feed_dict_train)\n",
    "        total_loss += loss\n",
    "        print('Epoch: {:04d}, total loss={:.9f}'.format(epoch+1, total_loss))\n",
    "    print ('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow variables\n",
    "features= len(sFeatures)\n",
    "X_place = tf.placeholder(tf.float32, shape=[None,features])\n",
    "y_place = tf.placeholder(tf.float32, shape=[None,2])\n",
    "W = tf.Variable(tf.random_normal([features, 2]), name='weights')\n",
    "b = tf.Variable(tf.zeros(2), name='bias')\n",
    "logits = tf.matmul(X_place, W) + b\n",
    "y_pred = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy\n",
    "learning_rate = 0.001\n",
    "cross_entropy = -tf.reduce_sum(y_place * tf.log(y_pred + 1e-10), axis=1)\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_train)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training starting option 1\n",
    "# for epoch in range(10000):\n",
    "#     total_loss = 0.\n",
    "#     x_batch, y_batch = next_batch(batch_size, X_train, y_train)\n",
    "#     feed_dict_train = {X_place: x_batch, y_place: y_batch}\n",
    "#     _, loss = sess.run([train_op, cost], feed_dict=feed_dict_train)\n",
    "#     total_loss += loss\n",
    "#     print('Epoch: {:04d}, total loss={:.9f}'.format(epoch+1, total_loss))\n",
    "# #     print('Epoch: {:04d}, total loss={:.9f}, entropy: {}'.format(epoch+1, total_loss, cross_entropy))\n",
    "# #     print('predict: {:.6f}'.format(y_pred))\n",
    "# print ('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, total loss=5541.680708885\n",
      "Epoch: 0002, total loss=5541.540865898\n",
      "Epoch: 0003, total loss=5541.413506508\n",
      "Epoch: 0004, total loss=5541.293797493\n",
      "Epoch: 0005, total loss=5541.177625656\n",
      "Epoch: 0006, total loss=5541.061377525\n",
      "Epoch: 0007, total loss=5540.940877914\n",
      "Epoch: 0008, total loss=5540.811052322\n",
      "Epoch: 0009, total loss=5540.664940834\n",
      "Epoch: 0010, total loss=5540.492752075\n",
      "Epoch: 0011, total loss=5540.281257629\n",
      "Epoch: 0012, total loss=5540.016325951\n",
      "Epoch: 0013, total loss=5539.691099167\n",
      "Epoch: 0014, total loss=5539.312497139\n",
      "Epoch: 0015, total loss=5538.894763947\n",
      "Epoch: 0016, total loss=5538.449623108\n",
      "Epoch: 0017, total loss=5537.984559059\n",
      "Epoch: 0018, total loss=5537.504734993\n",
      "Epoch: 0019, total loss=5537.014330864\n",
      "Epoch: 0020, total loss=5536.516591072\n",
      "Epoch: 0021, total loss=5536.014182091\n",
      "Epoch: 0022, total loss=5535.508934975\n",
      "Epoch: 0023, total loss=5535.002261162\n",
      "Epoch: 0024, total loss=5534.494976997\n",
      "Epoch: 0025, total loss=5533.987763405\n",
      "Epoch: 0026, total loss=5533.481121063\n",
      "Epoch: 0027, total loss=5532.975081444\n",
      "Epoch: 0028, total loss=5532.470190048\n",
      "Epoch: 0029, total loss=5531.963916779\n",
      "Epoch: 0030, total loss=5531.453557968\n",
      "Epoch: 0031, total loss=5530.930762291\n",
      "Epoch: 0032, total loss=5530.372378349\n",
      "Epoch: 0033, total loss=5529.686459541\n",
      "Epoch: 0034, total loss=5528.053771973\n",
      "Epoch: 0035, total loss=5522.468223572\n",
      "Epoch: 0036, total loss=4641.045194268\n",
      "Epoch: 0037, total loss=2276.907869339\n",
      "Epoch: 0038, total loss=2246.107505202\n",
      "Epoch: 0039, total loss=2245.609334469\n",
      "Epoch: 0040, total loss=2237.503407478\n",
      "Epoch: 0041, total loss=2234.535342216\n",
      "Epoch: 0042, total loss=2223.361686349\n",
      "Epoch: 0043, total loss=2201.085868716\n",
      "Epoch: 0044, total loss=2211.278185725\n",
      "Epoch: 0045, total loss=2201.278308153\n",
      "Epoch: 0046, total loss=2180.538972139\n",
      "Epoch: 0047, total loss=2165.551484466\n",
      "Epoch: 0048, total loss=2169.255427718\n",
      "Epoch: 0049, total loss=2153.792915821\n",
      "Epoch: 0050, total loss=2150.144356847\n",
      "Epoch: 0051, total loss=2153.173109412\n",
      "Epoch: 0052, total loss=2117.270372987\n",
      "Epoch: 0053, total loss=2128.635402679\n",
      "Epoch: 0054, total loss=2107.935560584\n",
      "Epoch: 0055, total loss=2136.825086594\n",
      "Epoch: 0056, total loss=2095.896182895\n",
      "Epoch: 0057, total loss=2089.871934175\n",
      "Epoch: 0058, total loss=2092.120839357\n",
      "Epoch: 0059, total loss=2087.946722388\n",
      "Epoch: 0060, total loss=2073.782358944\n",
      "Epoch: 0061, total loss=2069.579752207\n",
      "Epoch: 0062, total loss=2047.155754566\n",
      "Epoch: 0063, total loss=2041.778621674\n",
      "Epoch: 0064, total loss=2038.850537181\n",
      "Epoch: 0065, total loss=2045.356049776\n",
      "Epoch: 0066, total loss=2019.289085865\n",
      "Epoch: 0067, total loss=2000.971819818\n",
      "Epoch: 0068, total loss=1988.537428916\n",
      "Epoch: 0069, total loss=2023.096294761\n",
      "Epoch: 0070, total loss=1994.693944573\n",
      "Epoch: 0071, total loss=1995.632987082\n",
      "Epoch: 0072, total loss=1962.075517833\n",
      "Epoch: 0073, total loss=1980.374684215\n",
      "Epoch: 0074, total loss=1913.543035090\n",
      "Epoch: 0075, total loss=1985.007087588\n",
      "Epoch: 0076, total loss=1961.560680687\n",
      "Epoch: 0077, total loss=1959.330153108\n",
      "Epoch: 0078, total loss=1901.045905650\n",
      "Epoch: 0079, total loss=1936.409856021\n",
      "Epoch: 0080, total loss=1841.037038743\n",
      "Epoch: 0081, total loss=1747.929329813\n",
      "Epoch: 0082, total loss=1827.783097923\n",
      "Epoch: 0083, total loss=1832.915087700\n",
      "Epoch: 0084, total loss=1870.401968002\n",
      "Epoch: 0085, total loss=1873.972519338\n",
      "Epoch: 0086, total loss=1683.784416139\n",
      "Epoch: 0087, total loss=1697.369690478\n",
      "Epoch: 0088, total loss=1580.384665251\n",
      "Epoch: 0089, total loss=1591.970902562\n",
      "Epoch: 0090, total loss=1706.081723690\n",
      "Epoch: 0091, total loss=1579.657081068\n",
      "Epoch: 0092, total loss=1616.111249089\n",
      "Epoch: 0093, total loss=1543.525668681\n",
      "Epoch: 0094, total loss=1604.936437428\n",
      "Epoch: 0095, total loss=1542.160660446\n",
      "Epoch: 0096, total loss=1424.783847332\n",
      "Epoch: 0097, total loss=1582.265097320\n",
      "Epoch: 0098, total loss=1511.569693327\n",
      "Epoch: 0099, total loss=1565.405283630\n",
      "Epoch: 0100, total loss=1483.009308040\n",
      "Training complete!\n",
      "Accuracy on validation: 49.25%\n"
     ]
    }
   ],
   "source": [
    "# training starting option 1\n",
    "for epoch in range(100):\n",
    "    total_loss = 0.\n",
    "    for i in range(len(X_train)):\n",
    "        feed = {X_place: X_train, y_place: y_train}\n",
    "        _, loss = sess.run([train_op, cost], feed_dict=feed)\n",
    "        total_loss += loss\n",
    "    print('Epoch: {:04d}, total loss={:.9f}'.format(epoch+1, total_loss))\n",
    "#     print('Epoch: {:04d}, total loss={:.9f}, entropy: {}'.format(epoch+1, total_loss, cross_entropy))\n",
    "#     print('predict: {:.6f}'.format(y_pred))\n",
    "print ('Training complete!')\n",
    "\n",
    "feed_dict_train = {X_place: X_train}\n",
    "prediction = sess.run(y_pred, feed_dict=feed_dict_train)\n",
    "correct = np.equal(np.argmax(prediction,axis=1), np.argmax(y_train_data,axis=1))\n",
    "# evaluate the accuracy\n",
    "accuracy = np.mean(correct)\n",
    "print('Accuracy on validation: {:2.2%}'.format (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, model_path+'Titanic_model.ckpt')\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/lambert/Documents/Python_code/tensorflow_code/Titanic/Titanic_model/Titanic_model.ckpt\n",
      "Accuracy on validation: 63.06%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the training data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path+'Titanic_model.ckpt')\n",
    "    feed_dict_train = {X_place: X_train_data}\n",
    "    prediction = sess.run(y_pred, feed_dict=feed_dict_train)\n",
    "    correct = np.equal(np.argmax(prediction,axis=1), np.argmax(y_train_data,axis=1))\n",
    "# evaluate the accuracy\n",
    "accuracy = np.mean(correct)\n",
    "print('Accuracy on validation: {:2.2%}'.format (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/lambert/Documents/Python_code/tensorflow_code/Titanic/Titanic_model/Titanic_model.ckpt\n",
      "Accuracy on validation: 64.13%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the validate data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path+'Titanic_model.ckpt')\n",
    "    feed_dict_valid = {X_place: X_val_data}\n",
    "    prediction = sess.run(y_pred, feed_dict=feed_dict_valid)\n",
    "    correct = np.equal(np.argmax(prediction,axis=1), np.argmax(y_val_data,axis=1))\n",
    "# evaluate the accuracy\n",
    "\n",
    "accuracy = np.mean(correct)\n",
    "print('Accuracy on validation: {:2.2%}'.format (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/lambert/Documents/Python_code/tensorflow_code/Titanic/Titanic_model/Titanic_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# prediction for test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path+'Titanic_model.ckpt')\n",
    "    feed_dict_test = {X_place: X_test}\n",
    "    prediction = sess.run(y_pred, feed_dict=feed_dict_test)\n",
    "    y_predict = np.argmax(prediction, axis=1)\n",
    "# evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passengerid = np.array(df_test.PassengerId.tolist())\n",
    "submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "submission.to_csv('submission_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418,)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-3089a8a16460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# plot_example_errors()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# optimize(num_iterations=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot_example_errors()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# plot_weights()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# print_accuracy()\n",
    "# plot_example_errors()\n",
    "# optimize(num_iterations=10)\n",
    "print_accuracy()\n",
    "# plot_example_errors()\n",
    "# plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_Writer=tf.summary.FileWriter(''/Users/lambert/Documents/Python_code/tensorflow_code/Titanic'\n",
    "                                  ,sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(C=1, max_iter=100, solver='lbfgs').fit(X_train, y_train)\n",
    "yp = lr.predict(X_train)\n",
    "y_predict = lr.predict(X_test)\n",
    "\n",
    "passengerid = np.array(df_test.PassengerId.tolist())\n",
    "submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "submission.to_csv('submission_knn.csv')\n",
    "# passengerid = np.array(df_train.PassengerId.tolist())\n",
    "# submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': yp}).set_index('PassengerId')\n",
    "# submission.to_csv('y_train_predict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "# gbdt = GradientBoostingClassifier(n_estimators=300, max_depth=5).fit(X_train, y_train)\n",
    "# y_predict = gbdt.predict(X_test)\n",
    "\n",
    "# passengerid = np.array(df_test.PassengerId.tolist())\n",
    "# submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "# submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_depth=3).fit(X_train, y_train)\n",
    "y_predict = rfc.predict(X_test)\n",
    "passengerid = np.array(df_test.PassengerId.tolist())\n",
    "submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "submission.to_csv('submission_random.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SDG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (SGD) easily scale with more than 10^5 samples and features\n",
    "sgd = SGDClassifier(alpha=0.01, max_iter=1000).fit(X_train, y_train)\n",
    "y_predict = sgd.predict(X_test)\n",
    "passengerid = np.array(df_test.PassengerId.tolist())\n",
    "submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "submission.to_csv('submission_sdg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
