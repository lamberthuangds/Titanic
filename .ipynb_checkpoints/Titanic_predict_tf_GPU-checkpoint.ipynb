{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lambert/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.client import device_lib\n",
    "model_path = '~/Git/Titanic/Titanic_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_title(name):\n",
    "    if pd.isnull(name):\n",
    "        return 'Null'\n",
    "    title_search = re.search('([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1).lower()\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {'mr':1, 'mrs':2, 'mme':2, 'ms':3, 'miss':3, 'mlle':3, 'don':4, 'sir':4, 'jonkheer':4,\n",
    "          'major':4, 'col':4, 'dr':4, 'master':4, 'capt':4, 'dona':5, 'lady':5, 'countess':5,\n",
    "         'rev':6}\n",
    "df_train['Title'] = df_train['Name'].apply(lambda name: titles.get(get_title(name)))\n",
    "df_train['Honor'] = df_train['Title'].apply(lambda title: 1 if title ==4 or title == 5 else 0)\n",
    "df_test['Title'] = df_test['Name'].apply(lambda name: titles.get(get_title(name)))\n",
    "df_test['Honor'] = df_test['Title'].apply(lambda title: 1 if title ==4 or title == 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambert/virtualenvs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# 1. Data clearning\n",
    "# df_train.Age.fillna(fill_df_Age, inplace=True)\n",
    "# df_test.Age.fillna(fill_df_Age, inplace=True)\n",
    "for i in range(1,7):\n",
    "    a = df_train[(df_train.Age.isnull()) & (df_train.Title==i)]\n",
    "    df_train.Age.iloc[a.index] = df_train.Age[df_train.Title==i].median()\n",
    "    b = df_test[(df_test.Age.isnull()) & (df_test.Title==i)]\n",
    "    df_test.Age.iloc[b.index] = df_train.Age[df_train.Title==i].median()\n",
    "\n",
    "# Cabin - fill nan with 'fill' and select first cabin\n",
    "df_train.Cabin.fillna('fill',inplace=True)\n",
    "df_train.Cabin = df_train.Cabin.map(lambda x:x.split(' ')[0])\n",
    "df_test.Cabin.fillna('fill', inplace=True)\n",
    "df_test.Cabin = df_test.Cabin.map(lambda x:x.split(' ')[0])\n",
    "\n",
    "# Sex - male: 1, female: 1\n",
    "df_train.Sex.replace({'male':1, 'female':0}, inplace=True)\n",
    "df_test.Sex.replace({'male':1, 'female':0}, inplace=True)\n",
    "\n",
    "df_train = df_train.dropna(axis=0, how='any')\n",
    "df_test.Fare.fillna(df_test.Fare.mean(), inplace=True)\n",
    "\n",
    "# Create a 'Deceased' column for sconed class\n",
    "df_train['Deceased'] = df_train.Survived.apply(lambda s: int(not s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Select the features\n",
    "sFeatures = ['Pclass',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',             \n",
    " 'Cabin',             \n",
    " 'Embarked',\n",
    " 'Title',\n",
    " 'Honor']\n",
    "\n",
    "X = df_train[sFeatures]\n",
    "y = df_train[['Survived','Deceased']]\n",
    "y = y.values.reshape(len(y),2)\n",
    "X_test = df_test[sFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambert/virtualenvs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# 3. Label Encoder\n",
    "# X_test has elements that X doesn't have\n",
    "# create a Cabin_labels cover X and X_test\n",
    "X_Cabin_unique = X['Cabin'].unique()\n",
    "X_lack = X_test.Cabin[X_test['Cabin'].isin(X_Cabin_unique)==0].values\n",
    "Cabin_labels = np.append(X_Cabin_unique, X_lack)\n",
    "# print(X_Cabin_unique.shape, X_Cabin_unique)\n",
    "# print(Cabin_labels.shape, Cabin_labels)\n",
    "\n",
    "le_Cabin = LabelEncoder().fit(Cabin_labels)\n",
    "le_Embarked = LabelEncoder().fit(np.array(X['Embarked'].tolist()))\n",
    "\n",
    "X.loc[:,'Cabin'] = le_Cabin.transform(X['Cabin'])\n",
    "X.loc[:,'Embarked'] = le_Embarked.transform(X['Embarked'])\n",
    "X_test.loc[:,'Cabin'] = le_Cabin.transform(X_test['Cabin'])\n",
    "X_test.loc[:,'Embarked'] = le_Embarked.transform(X_test['Embarked'])\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Honor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.287500</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.350000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.587500</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.741700</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.854200</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.075001</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.925000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  SibSp  Parch       Fare  Cabin  Embarked  Title  Honor\n",
       "242     2.0  1.0  29.0    0.0    0.0  10.500000  180.0       2.0    1.0    0.0\n",
       "84      2.0  0.0  17.0    0.0    0.0  10.500000  180.0       2.0    3.0    0.0\n",
       "512     1.0  1.0  36.0    0.0    0.0  26.287500  152.0       2.0    1.0    0.0\n",
       "855     3.0  0.0  18.0    0.0    1.0   9.350000  180.0       2.0    2.0    0.0\n",
       "503     3.0  0.0  37.0    0.0    0.0   9.587500  180.0       2.0    3.0    0.0\n",
       "381     3.0  0.0   1.0    0.0    2.0  15.741700  180.0       0.0    3.0    0.0\n",
       "196     3.0  1.0  30.0    0.0    0.0   7.750000  180.0       1.0    1.0    0.0\n",
       "396     3.0  0.0  31.0    0.0    0.0   7.854200  180.0       2.0    3.0    0.0\n",
       "567     3.0  0.0  29.0    0.0    4.0  21.075001  180.0       2.0    2.0    0.0\n",
       "400     3.0  1.0  39.0    0.0    0.0   7.925000  180.0       2.0    1.0    0.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch, X, y):\n",
    "    num = np.arange(len(X))\n",
    "    np.random.shuffle(num)\n",
    "    select = num[0:batch]\n",
    "    x_batch = X.iloc[select]\n",
    "    y_batch = y[select]\n",
    "    \n",
    "    return(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    total_loss = 0\n",
    "    for i in range(num_iterations):\n",
    "        x_batch, y_batch = next_batch(batch_size, X_train_data, y_train_data)\n",
    "        feed_dict_train = {X_place: x_batch, y_place: y_batch}\n",
    "        _, loss = sess.run([train_op, cost], feed_dict=feed_dict_train)\n",
    "        total_loss += loss\n",
    "        print('Epoch: {:04d}, total loss={:.9f}'.format(epoch+1, total_loss))\n",
    "    print ('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow variables\n",
    "features= len(sFeatures)\n",
    "X_place = tf.placeholder(tf.float32, shape=[None,features])\n",
    "y_place = tf.placeholder(tf.float32, shape=[None,2])\n",
    "W = tf.Variable(tf.random_normal([features, 2]), name='weights')\n",
    "b = tf.Variable(tf.zeros(2), name='bias')\n",
    "logits = tf.matmul(X_place, W) + b\n",
    "y_pred = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy\n",
    "learning_rate = 0.001\n",
    "cross_entropy = -tf.reduce_sum(y_place * tf.log(y_pred + 1e-10), axis=1)\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_train)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training starting option 1\n",
    "# for epoch in range(10000):\n",
    "#     total_loss = 0.\n",
    "#     x_batch, y_batch = next_batch(batch_size, X_train, y_train)\n",
    "#     feed_dict_train = {X_place: x_batch, y_place: y_batch}\n",
    "#     _, loss = sess.run([train_op, cost], feed_dict=feed_dict_train)\n",
    "#     total_loss += loss\n",
    "#     print('Epoch: {:04d}, total loss={:.9f}'.format(epoch+1, total_loss))\n",
    "# #     print('Epoch: {:04d}, total loss={:.9f}, entropy: {}'.format(epoch+1, total_loss, cross_entropy))\n",
    "# #     print('predict: {:.6f}'.format(y_pred))\n",
    "# print ('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training starting option 1 with CPU\n",
    "# for epoch in range(100):\n",
    "#     total_loss = 0.\n",
    "#     for i in range(len(X_train)):\n",
    "#         feed = {X_place: X_train, y_place: y_train}\n",
    "#         _, loss = sess.run([train_op, cost], feed_dict=feed)\n",
    "#         total_loss += loss\n",
    "#     print('Epoch: {:04d}, total loss={:.9f}'.format(epoch+1, total_loss))\n",
    "# #     print('Epoch: {:04d}, total loss={:.9f}, entropy: {}'.format(epoch+1, total_loss, cross_entropy))\n",
    "# #     print('predict: {:.6f}'.format(y_pred))\n",
    "# print ('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, total loss=4320.630758524\n",
      "Epoch: 0002, total loss=2955.140067697\n",
      "Epoch: 0003, total loss=2727.742041349\n",
      "Epoch: 0004, total loss=2681.316459656\n",
      "Epoch: 0005, total loss=2677.858217001\n",
      "Epoch: 0006, total loss=2623.450826406\n",
      "Epoch: 0007, total loss=2613.135359883\n",
      "Epoch: 0008, total loss=2587.414885759\n",
      "Epoch: 0009, total loss=2564.683841348\n",
      "Epoch: 0010, total loss=2525.211843848\n",
      "Epoch: 0011, total loss=2513.409255743\n",
      "Epoch: 0012, total loss=2455.071187615\n",
      "Epoch: 0013, total loss=2425.298911214\n",
      "Epoch: 0014, total loss=2373.237485051\n",
      "Epoch: 0015, total loss=2327.306004882\n",
      "Epoch: 0016, total loss=2281.301659465\n",
      "Epoch: 0017, total loss=2218.327041626\n",
      "Epoch: 0018, total loss=2202.186666012\n",
      "Epoch: 0019, total loss=2169.721481442\n",
      "Epoch: 0020, total loss=2107.359412611\n",
      "Epoch: 0021, total loss=2069.546857417\n",
      "Epoch: 0022, total loss=2048.524602354\n",
      "Epoch: 0023, total loss=2045.587014437\n",
      "Epoch: 0024, total loss=1936.994925499\n",
      "Epoch: 0025, total loss=1924.541502118\n",
      "Epoch: 0026, total loss=1883.808007658\n",
      "Epoch: 0027, total loss=1836.338986635\n",
      "Epoch: 0028, total loss=1854.759538531\n",
      "Epoch: 0029, total loss=1842.072603285\n",
      "Epoch: 0030, total loss=1901.941935241\n",
      "Epoch: 0031, total loss=1856.716528416\n",
      "Epoch: 0032, total loss=1790.358456671\n",
      "Epoch: 0033, total loss=1766.425715923\n",
      "Epoch: 0034, total loss=1746.156539738\n",
      "Epoch: 0035, total loss=1627.586328506\n",
      "Epoch: 0036, total loss=1631.386473417\n",
      "Epoch: 0037, total loss=1552.863273025\n",
      "Epoch: 0038, total loss=1549.748161137\n",
      "Epoch: 0039, total loss=1509.383220434\n",
      "Epoch: 0040, total loss=1487.529814720\n",
      "Epoch: 0041, total loss=1434.373447835\n",
      "Epoch: 0042, total loss=1429.869387448\n",
      "Epoch: 0043, total loss=1399.336837769\n",
      "Epoch: 0044, total loss=1452.575550079\n",
      "Epoch: 0045, total loss=1405.517675936\n",
      "Epoch: 0046, total loss=1399.291665196\n",
      "Epoch: 0047, total loss=1251.920344710\n",
      "Epoch: 0048, total loss=1325.563328385\n",
      "Epoch: 0049, total loss=1343.304326653\n",
      "Epoch: 0050, total loss=1305.679369926\n",
      "Epoch: 0051, total loss=1309.318366587\n",
      "Epoch: 0052, total loss=1222.625429690\n",
      "Epoch: 0053, total loss=1377.775260806\n",
      "Epoch: 0054, total loss=1287.906028509\n",
      "Epoch: 0055, total loss=1179.783558428\n",
      "Epoch: 0056, total loss=1236.204088330\n",
      "Epoch: 0057, total loss=1245.935040712\n",
      "Epoch: 0058, total loss=1305.335820198\n",
      "Epoch: 0059, total loss=1233.217427492\n",
      "Epoch: 0060, total loss=1182.319359004\n",
      "Epoch: 0061, total loss=1268.441324294\n",
      "Epoch: 0062, total loss=1295.426664352\n",
      "Epoch: 0063, total loss=1264.737851381\n",
      "Epoch: 0064, total loss=1300.668173075\n",
      "Epoch: 0065, total loss=1194.830670118\n",
      "Epoch: 0066, total loss=1246.777820647\n",
      "Epoch: 0067, total loss=1192.475024164\n",
      "Epoch: 0068, total loss=1358.756620109\n",
      "Epoch: 0069, total loss=1238.406175196\n",
      "Epoch: 0070, total loss=1351.836696982\n",
      "Epoch: 0071, total loss=1226.932404220\n",
      "Epoch: 0072, total loss=1291.038160205\n",
      "Epoch: 0073, total loss=1267.432329476\n",
      "Epoch: 0074, total loss=1216.040628731\n",
      "Epoch: 0075, total loss=1235.056888402\n",
      "Epoch: 0076, total loss=1317.005719543\n",
      "Epoch: 0077, total loss=1191.076560140\n",
      "Epoch: 0078, total loss=1220.571953893\n",
      "Epoch: 0079, total loss=1234.032121837\n",
      "Epoch: 0080, total loss=1310.452685714\n",
      "Epoch: 0081, total loss=1222.411359787\n",
      "Epoch: 0082, total loss=1238.451747358\n",
      "Epoch: 0083, total loss=1167.417903423\n",
      "Epoch: 0084, total loss=1213.854532242\n",
      "Epoch: 0085, total loss=1194.826440275\n",
      "Epoch: 0086, total loss=1206.040358961\n",
      "Epoch: 0087, total loss=1283.555472493\n",
      "Epoch: 0088, total loss=1219.940647006\n",
      "Epoch: 0089, total loss=1237.987528145\n",
      "Epoch: 0090, total loss=1257.551064491\n",
      "Epoch: 0091, total loss=1256.159332395\n",
      "Epoch: 0092, total loss=1241.790041447\n",
      "Epoch: 0093, total loss=1176.316393495\n",
      "Epoch: 0094, total loss=1241.578260481\n",
      "Epoch: 0095, total loss=1202.001205802\n",
      "Epoch: 0096, total loss=1143.440814734\n",
      "Epoch: 0097, total loss=1222.145639122\n",
      "Epoch: 0098, total loss=1195.684398234\n",
      "Epoch: 0099, total loss=1212.650160611\n",
      "Epoch: 0100, total loss=1207.531252265\n",
      "Training complete!\n",
      "Accuracy on validation: 65.47%\n"
     ]
    }
   ],
   "source": [
    "# training starting option 1 with GPU\n",
    "# with tf.device('/device:GPU:0'):\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0.\n",
    "        for i in range(len(X_train)):\n",
    "            feed = {X_place: X_train, y_place: y_train}\n",
    "            _, loss = sess.run([train_op, cost], feed_dict=feed)\n",
    "            total_loss += loss\n",
    "        print('Epoch: {:04d}, total loss={:.9f}'.format(epoch+1, total_loss))\n",
    "    #     print('Epoch: {:04d}, total loss={:.9f}, entropy: {}'.format(epoch+1, total_loss, cross_entropy))\n",
    "    #     print('predict: {:.6f}'.format(y_pred))\n",
    "    print ('Training complete!')\n",
    "\n",
    "    feed_dict_train = {X_place: X_train}\n",
    "    prediction = sess.run(y_pred, feed_dict=feed_dict_train)\n",
    "    correct = np.equal(np.argmax(prediction,axis=1), np.argmax(y_train,axis=1))\n",
    "    # evaluate the accuracy\n",
    "    accuracy = np.mean(correct)\n",
    "    print('Accuracy on validation: {:2.2%}'.format (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, model_path+'Titanic_model.ckpt')\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/lambert/Documents/Python_code/tensorflow_code/Titanic/Titanic_model/Titanic_model.ckpt\n",
      "Accuracy on validation: 63.06%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the training data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path+'Titanic_model.ckpt')\n",
    "    feed_dict_train = {X_place: X_train_data}\n",
    "    prediction = sess.run(y_pred, feed_dict=feed_dict_train)\n",
    "    correct = np.equal(np.argmax(prediction,axis=1), np.argmax(y_train_data,axis=1))\n",
    "# evaluate the accuracy\n",
    "accuracy = np.mean(correct)\n",
    "print('Accuracy on validation: {:2.2%}'.format (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/lambert/Documents/Python_code/tensorflow_code/Titanic/Titanic_model/Titanic_model.ckpt\n",
      "Accuracy on validation: 64.13%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the validate data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path+'Titanic_model.ckpt')\n",
    "    feed_dict_valid = {X_place: X_val_data}\n",
    "    prediction = sess.run(y_pred, feed_dict=feed_dict_valid)\n",
    "    correct = np.equal(np.argmax(prediction,axis=1), np.argmax(y_val_data,axis=1))\n",
    "# evaluate the accuracy\n",
    "\n",
    "accuracy = np.mean(correct)\n",
    "print('Accuracy on validation: {:2.2%}'.format (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/lambert/Documents/Python_code/tensorflow_code/Titanic/Titanic_model/Titanic_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# prediction for test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_path+'Titanic_model.ckpt')\n",
    "    feed_dict_test = {X_place: X_test}\n",
    "    prediction = sess.run(y_pred, feed_dict=feed_dict_test)\n",
    "    y_predict = np.argmax(prediction, axis=1)\n",
    "# evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passengerid = np.array(df_test.PassengerId.tolist())\n",
    "submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "submission.to_csv('submission_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418,)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-3089a8a16460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# plot_example_errors()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# optimize(num_iterations=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot_example_errors()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# plot_weights()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# print_accuracy()\n",
    "# plot_example_errors()\n",
    "# optimize(num_iterations=10)\n",
    "print_accuracy()\n",
    "# plot_example_errors()\n",
    "# plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_Writer=tf.summary.FileWriter(''/Users/lambert/Documents/Python_code/tensorflow_code/Titanic'\n",
    "                                  ,sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/device:GPU:0'):\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate as much GPU memory based on runtime allocations\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate 40% memory of GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically choose existing in case specified one doesn't exit\n",
    "# allow_soft_placement=True\n",
    "with tf.device('/device:GPU:2'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "# Creates a session with allow_soft_placement and log_device_placement set\n",
    "# to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lambert/Library/Jupyter/runtime/kernel-d05fde7f-2b3b-40a0-9219-fbcb703d8987.json'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 2731858689698339022]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(C=1, max_iter=100, solver='lbfgs').fit(X_train, y_train)\n",
    "yp = lr.predict(X_train)\n",
    "y_predict = lr.predict(X_test)\n",
    "\n",
    "passengerid = np.array(df_test.PassengerId.tolist())\n",
    "submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "submission.to_csv('submission_knn.csv')\n",
    "# passengerid = np.array(df_train.PassengerId.tolist())\n",
    "# submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': yp}).set_index('PassengerId')\n",
    "# submission.to_csv('y_train_predict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "# gbdt = GradientBoostingClassifier(n_estimators=300, max_depth=5).fit(X_train, y_train)\n",
    "# y_predict = gbdt.predict(X_test)\n",
    "\n",
    "# passengerid = np.array(df_test.PassengerId.tolist())\n",
    "# submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "# submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_depth=3).fit(X_train, y_train)\n",
    "y_predict = rfc.predict(X_test)\n",
    "passengerid = np.array(df_test.PassengerId.tolist())\n",
    "submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "submission.to_csv('submission_random.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SDG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (SGD) easily scale with more than 10^5 samples and features\n",
    "sgd = SGDClassifier(alpha=0.01, max_iter=1000).fit(X_train, y_train)\n",
    "y_predict = sgd.predict(X_test)\n",
    "passengerid = np.array(df_test.PassengerId.tolist())\n",
    "submission = pd.DataFrame({'PassengerId': passengerid, 'Survived': y_predict}).set_index('PassengerId')\n",
    "submission.to_csv('submission_sdg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
